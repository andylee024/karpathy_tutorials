{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Karpathy Lecture 4 - backpropogation\n",
    "This lecture focuses on implementing backward pass (gradients) from scratch. We will define our gradients and then compare them with pytorch's autograd values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "abf335d820294e95bbd904d18c016305",
    "deepnote_cell_type": "code",
    "execution_context_id": "cc404b44-5174-4c28-a449-e555cb51f618",
    "execution_millis": 903,
    "execution_start": 1739751089677,
    "source_hash": "205c2617"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4945665137f944dcb168dda590c42bce",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "9fd33cb089a44c4cbb7d9b6665c45f5f",
    "deepnote_cell_type": "code",
    "execution_context_id": "cc404b44-5174-4c28-a449-e555cb51f618",
    "execution_millis": 1,
    "execution_start": 1739751090777,
    "source_hash": "1c8a8db3"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(words, block_size=3):\n",
    "    \"\"\"Takes a list of words and block size and prepares dataset for NN training\"\"\"\n",
    "\n",
    "    # build dataset\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        word = w + \".\"\n",
    "\n",
    "        for ch in word:\n",
    "            ch_idx = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ch_idx)\n",
    "\n",
    "            # update context\n",
    "            new_context = context[1:] + [ch_idx]\n",
    "            context = new_context\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "# intake raw data\n",
    "path = \"/Users/andylee/Projects/ml-tutorials/data/names.txt\"\n",
    "words = open(path, 'r').read().splitlines()\n",
    "\n",
    "# build lookup tables\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "# split dataset\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "block_size = 3\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "Xtr,  Ytr  = prepare_dataset(words[:n1], block_size=block_size)     # 80%\n",
    "Xdev, Ydev = prepare_dataset(words[n1:n2], block_size=block_size)   # 10%\n",
    "Xte,  Yte  = prepare_dataset(words[n2:], block_size=block_size)     # 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "73043790562445e9bf2e1fda7e8a5d59",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": []
   },
   "source": [
    "# Build network\n",
    "This section implements a simple MLP with batchnorm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "43cb513286e34610a76a20fbd81c6137",
    "deepnote_cell_type": "code",
    "execution_context_id": "cc404b44-5174-4c28-a449-e555cb51f618",
    "execution_millis": 1,
    "execution_start": 1739751091277,
    "source_hash": "5712460a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total network parameters: 12097\n"
     ]
    }
   ],
   "source": [
    "# set seed\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "# hyperparameters\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "# initialization parameters\n",
    "kaiming_initialization = (5/3)/((n_embd * block_size)**0.5)\n",
    "epsilon_initialization = 0.01\n",
    "\n",
    "# network - embedding\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "\n",
    "# network - mlp\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * kaiming_initialization\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * epsilon_initialization\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * epsilon_initialization\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * epsilon_initialization\n",
    "\n",
    "# network - batchnorm\n",
    "batchnorm_gain = torch.ones((1, n_hidden))\n",
    "batchnorm_bias = torch.zeros((1, n_hidden))\n",
    "batchnorm_mean_running = torch.zeros((1, n_hidden))\n",
    "batchnorm_var_running = torch.ones((1, n_hidden))\n",
    "\n",
    "# network parameters to optimize\n",
    "parameters = [C, W1, W2, b2, batchnorm_gain, batchnorm_bias]\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "total_parameters = sum(p.nelement() for p in parameters) # number of parameters in total\n",
    "print(f\" Total network parameters: {total_parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Loop\n",
    "This section builds the optimization loop for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 20000\n",
    "batch_size = 2\n",
    "n = batch_size # for shorter \n",
    "vocab_size = vocab_size\n",
    "\n",
    "lossi = []\n",
    "for i in range(max_steps):\n",
    "    \n",
    "    # generate mini-batch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]  # batch X,Y\n",
    "\n",
    "    # forward pass - embedding layer\n",
    "    emb = C[Xb]  # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1)  # concatenate the vectors\n",
    "    \n",
    "    # forward pass - linear layer\n",
    "    linear_out = embcat @ W1 + b1\n",
    "\n",
    "    # forward pass - batchnorm\n",
    "    batch_mean = (1/n) * linear_out.sum(0, keepdim=True)\n",
    "    batch_variance = 1/(n-1) * ((linear_out - batch_mean)**2).sum(0, keepdim=True) \n",
    "    batch_std_inverse = (batch_variance + 1e-5)**-0.5\n",
    "    batchnorm_out = (linear_out - batch_mean) * batch_std_inverse\n",
    "    batchnorm_out = batchnorm_gain * batchnorm_out + batchnorm_bias\n",
    "    \n",
    "    # log running means & variances\n",
    "    with torch.no_grad():\n",
    "        bnmean_running = 0.999 * batchnorm_mean_running + 0.001 * batch_mean\n",
    "        bnstd_running = 0.999 * batchnorm_var_running + 0.001 * batch_variance\n",
    "\n",
    "    # forward pass - non-linear\n",
    "    hidden_out = torch.tanh(batchnorm_out)\n",
    "\n",
    "    # forward pass - softmax \n",
    "    logits = hidden_out @ W2 + b2  # output layer\n",
    "    logits_maxes = logits.max(1, keepdim=True).values # numerical stability hack\n",
    "    logits_normalized = logits - logits_maxes\n",
    "\n",
    "    # forward pass - softmax\n",
    "    counts = torch.exp(logits_normalized)\n",
    "    counts_sum = counts.sum(1, keepdim=True) # output (m, 1)\n",
    "    counts_sum_inverse = counts_sum ** -1\n",
    "    probabilities = counts * counts_sum_inverse\n",
    "\n",
    "    # forward pass - log probabilities\n",
    "    log_probabilities = probabilities.log()\n",
    "\n",
    "    # forward pass - compute loss\n",
    "    # E.g. 0th batch row, Yb=5 (e.g. E), then you take neg log probabilty of E and calculate loss\n",
    "    cross_entropy_loss = -log_probabilities[range(n), Yb].mean(0, keepdim=True)\n",
    "\n",
    "    # capture intermediate gradients for inspection\n",
    "    for t in [emb, \n",
    "              batchnorm_out, \n",
    "              logits,\n",
    "              logits_maxes,\n",
    "              logits_normalized,\n",
    "              counts,\n",
    "              counts_sum,\n",
    "              counts_sum_inverse,\n",
    "              probabilities,\n",
    "              log_probabilities]:\n",
    "\n",
    "        t.retain_grad()\n",
    "\n",
    "    # backward pass \n",
    "    cross_entropy_loss.backward()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_probabilities | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# backward pass - manual gradient calculations\n",
    "\n",
    "# intermediaries\n",
    "Yb_one_hot = F.one_hot(Yb, num_classes=vocab_size)\n",
    "\n",
    "# gradient calculations\n",
    "dlog_probabilities = -1.0/batch_size * Yb_one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute gradients\n",
    "This section manually calculates gradients for backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: False | approximate: False | maxdiff: 25.15879249572754\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "# -----------------\n",
    "\n",
    "cmp('logprobs', dlog_probabilities, log_probabilities)\n",
    "# cmp('probs', dprobs, probs)\n",
    "# cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "# cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "# cmp('counts', dcounts, counts)\n",
    "# cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "# cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "# cmp('logits', dlogits, logits)\n",
    "# cmp('h', dh, h)\n",
    "# cmp('W2', dW2, W2)\n",
    "# cmp('b2', db2, b2)\n",
    "# cmp('hpreact', dhpreact, hpreact)\n",
    "# cmp('bngain', dbngain, bngain)\n",
    "# cmp('bnbias', dbnbias, bnbias)\n",
    "# cmp('bnraw', dbnraw, bnraw)\n",
    "# cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "# cmp('bnvar', dbnvar, bnvar)\n",
    "# cmp('bndiff2', dbndiff2, bndiff2)\n",
    "# cmp('bndiff', dbndiff, bndiff)\n",
    "# cmp('bnmeani', dbnmeani, bnmeani)\n",
    "# cmp('hprebn', dhprebn, hprebn)\n",
    "# cmp('embcat', dembcat, embcat)\n",
    "# cmp('W1', dW1, W1)\n",
    "# cmp('b1', db1, b1)\n",
    "# cmp('emb', demb, emb)\n",
    "# cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "37a1a8542f7c48ed9006040d75bb593e",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": []
   },
   "source": [
    "## Pytorchify code\n",
    "This section converts raw network into modules that look like pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "cell_id": "b02746c38b704e22906d4225846d9e7d",
    "deepnote_cell_type": "code",
    "execution_context_id": "cc404b44-5174-4c28-a449-e555cb51f618",
    "execution_millis": 0,
    "execution_start": 1739755465539,
    "source_hash": "ef863d0b"
   },
   "outputs": [],
   "source": [
    "class Linear: \n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.bias = bias \n",
    "        self.weight = torch.randn(fan_in, fan_out, generator=g) / fan_in ** 0.5\n",
    "        self.b = torch.zeros(fan_out) if bias else None\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        if self.bias: \n",
    "            self.out = x @ self.weight + self.b\n",
    "        else:\n",
    "            self.out = x * self.weight\n",
    "        \n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight, self.b] if self.bias else [self.weight]\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.gamma = torch.randn(dim)\n",
    "        self.beta = torch.randn(dim)\n",
    "\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_std = torch.ones(dim)\n",
    "\n",
    "    def __call(self, x):\n",
    "        if self.training:\n",
    "            # calculate batch statistics\n",
    "            batch_mean = x.mean(0, keepdim=True)\n",
    "            batch_var = x.var(0, keepdim=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1-self.momentum) * self.running_mean + self.momentum * batch_mean\n",
    "                self.running_var = (1-self.momentum) * self.running_var + self.momentum * batch_var\n",
    "\n",
    "        else:\n",
    "            batch_mean = self.running_mean\n",
    "            batch_std = self.running_std\n",
    "        \n",
    "        xhat = (x - batch_mean) / torch.sqrt(batch_var + self.eps) ** 0.5\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "533ea0ccc33740f78a468c12e055afa4",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": []
   },
   "source": [
    "### Build pytorch network\n",
    "This section builds the network using pytorch layers defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "cell_id": "64d08007c01c4ec5a5e8b885ba21bc73",
    "deepnote_cell_type": "code",
    "execution_context_id": "cc404b44-5174-4c28-a449-e555cb51f618",
    "execution_millis": 1,
    "execution_start": 1739755768777,
    "source_hash": "ac4409b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46497\n"
     ]
    }
   ],
   "source": [
    "# setup NN parameters\n",
    "\n",
    "# generator\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "# NN parameters\n",
    "n_vocab = vocab_size\n",
    "n_embd = 10 \n",
    "n_hidden = 100\n",
    "\n",
    "# define pytorch layers\n",
    "C = torch.randn(vocab_size, n_embd)\n",
    "layers = [\n",
    "    # stacked layers\n",
    "    Linear(n_embd*block_size, n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden), Tanh(),\n",
    "\n",
    "    # logits layer\n",
    "    Linear(n_hidden, vocab_size)\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # make last layer less confident\n",
    "    layers[-1].weight *= 0.1 \n",
    "\n",
    "    for layer in layers[:-1]:\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.weight *= 5/3  # Ensure layer.weight modification correctly applies only to tensors\n",
    "    \n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "aa5b2d5eab1d40188b1d1bd0c6ca0d2d",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": []
   },
   "source": [
    "### Setup optimization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "0735cd747816448d94b1051f84f7f9d1",
    "deepnote_cell_type": "code",
    "execution_context_id": "cc404b44-5174-4c28-a449-e555cb51f618",
    "execution_millis": 2196,
    "execution_start": 1739755779818,
    "source_hash": "a9114d9a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m emb\u001b[38;5;241m.\u001b[39mview(emb\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# reshape dimensions\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[1;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# loss calculation\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layers' is not defined"
     ]
    }
   ],
   "source": [
    "# Correct optimization loop\n",
    "max_steps = 20000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "    # generate mini-batch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "    # embed input\n",
    "    emb = C[Xb] # embed input\n",
    "    x = emb.view(emb.shape[0], -1) # reshape dimensions\n",
    "\n",
    "    # forward pass\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "\n",
    "    # loss calculation\n",
    "    loss = F.cross_entropy(x, Yb)\n",
    "\n",
    "    # backward pass\n",
    "    for layer in layers:\n",
    "        if hasattr(layer, 'out'):\n",
    "            layer.out.retain_grad()\n",
    "    \n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update parameters\n",
    "    learning_rate = 0.1 if i < 100000 else 0.01 \n",
    "    for p in parameters:\n",
    "        p.data += -learning_rate * p.grad\n",
    "      \n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    if i >= 1000:\n",
    "        break # AFTER_DEBUG: would take out obviously to run full optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d7a4dc70646f41a0a05871c73b98a04a",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": []
   },
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9c570974c50d4d78a22e7f742ce90a0b",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "These are diagnostic tools that Karpathy has introduced us for understanding whether or not the network is updating properly based on our optimization parameters and initialization of weights. It's a way to quickly introspect the network and find if there are things that are wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "4497574282914e31b425b0b93d9cfa6c",
    "deepnote_cell_type": "code",
    "execution_context_id": "cc404b44-5174-4c28-a449-e555cb51f618",
    "execution_millis": 1,
    "execution_start": 1739755786047,
    "source_hash": "6e63d0b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m      2\u001b[0m legends \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(layers[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, Tanh):\n\u001b[1;32m      6\u001b[0m         t \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mout\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layers' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "\n",
    "for i, layer in enumerate(layers[:-1]):\n",
    "    if isinstance(layer, Tanh):\n",
    "        t = layer.out\n",
    "        print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
    "        \n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "\n",
    "plt.legend(legends);\n",
    "plt.title('activation distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "59de21e06eb04c798ed81978353b909d",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=0129c86c-0221-47a9-9ff5-5a790f5a7462' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "93ceb713b42d4013bfdcacd07312672f",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
